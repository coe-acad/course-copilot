{
    "id": "mark-scheme-generator",
    "prompt": "Generate a numbered Scheme of Evaluation in this format, for the given Question X:\n\nQuestion X: <verbatim question>\n\nAnswer template: List the core concepts or their clear equivalents that a full answer must reference, arranged in logical order.\n\nMarking Scheme:\n\n(A marks) A bullet that describes the first key expectation (concept or application) in context.\n\n(B marks) A bullet for the second expectation.\n\n…\n\n(–C marks) \n\nEnsure that:\n\nThe sum of A + B + … equals the total marks for Question X. Keep A, B, C, … as integers only if the maximum marks for the question exceed 3. Try to maximize the number of bullets.\n\nAssessment Objectives are stated briefly at the top (for example, Conceptual Understanding, Application & Problem-Solving, Relevance & Specificity).\n\nPlease include a Notes section with each question’s mark scheme:\n\nOpen-ended questions: any one comprehensive, context-relevant answer can earn full marks.\n\nMinor calculation errors should not cost any marks if reasoning is sound.\n\nBullets must call out the concept or context the student needs to cover, but need not use verbatim phrasing—equivalent meaning is fine. Provide marks liberally if the student shows rich understanding even if not using the right terms.\n\n***  \n:warning: **IMPORTANT**: your **only** output must be a single JSON object (for single questions) or a JSON array of such objects (for batch), each with these keys:\n```json\n{\n  \"question_number\": <int>,\n  \"answer_template\": \"<string>\",\n  \"marking_scheme\": [\"<bullet 1>\", \"<bullet 2>\", …],\n  \"deductions\": [\"<bullet 1>\", …],\n  \"notes\": \"<string>\"\n}\n```\n\nFor batch requests, append the following before listing the questions:\n\nNow generate a JSON array of mark-scheme objects for each of the following questions. Make sure you give the marking scheme for all the questions from the question paper",
    "required_input_variables": []
}
