{
  "id": "evaluation",
  "prompt": "Evaluate the extracted answers against the mark scheme./n/nCRITICAL: Preserve the EXACT question_text from the mark scheme and student_answer from the extracted answers without modification, truncation, or summarization./n/n**INPUT STRUCTURE:**/n/nEach answer sheet contains:/n- question_number: The question identifier/n- student_answer: FULL COMPLETE student response (preserve exactly as provided)/n/nEach item in the mark scheme data has:/n- questionnumber (or question_number): Question identifier/n- question-text (or question_text): The actual question being asked/n- answertemplate (or answer_template): The correct/expected answer/n- markingscheme (or marking_scheme): Grading criteria/n/n**OUTPUT STRUCTURE - CRITICAL FIELD MAPPING:**/n/nFor EACH student answer, you MUST populate these fields in your JSON output:/n/n1. question_text: Copy EXACTLY from the mark scheme's \"question-text\" or \"question_text\" field (this is the actual question, NOT the student's answer)/n2. student_answer: Copy EXACTLY from the answer sheet's \"student_answer\" field (this is what the student wrote, NOT the question text)/n3. correct_answer: Copy from the mark scheme's \"answertemplate\" or \"answer_template\" field (this is the expected correct answer)/n4. question_number: The question identifier/n5. score: Numeric score awarded/n6. max_score: Maximum possible score from marking scheme/n7. feedback: One-sentence justification/n/n**CRITICAL:** DO NOT swap or confuse these fields. The student_answer from the input goes to student_answer in output, and question-text from mark scheme goes to question_text in output./n/nCompare the student's answer in {{answer_sheets}} to the answer_template and marking_scheme. Award marks based on accuracy, completeness, and relevance./n/nGrade on correctness, completeness, and reasoning—not verbatim match to the scheme. Ignore minor slips (arithmetic/rounding) that don't change the method/result; deduct for conceptual errors, contradictions, or irrelevant meandering without clarity. But don't penalize verbosity if the required points are clearly made. Give proportional partial credit for partial answers; if a specific method/form is required, reserve full marks for meeting it while still granting partial credit to sound alternatives. Blank or non-answers get 0. Return a numeric score and one-sentence justification (what's correct + why any deductions)./n/nExpect answer detail proportional to max_marks. Example: for a 1-mark item, a single correct statement/value suffices—don't require extra explanation./n/nIMAGE HANDLING: Student answers may be images (diagrams, drawings, calculations, graphs, flowcharts, etc.). When you see {\"type\":\"image\",\"file_id\":\"file-abc123\"} in the answer without any accompanying text, treat it as a complete answer—not supplementary material. Thoroughly inspect the image, evaluate it against the marking criteria, and award marks based on what the image shows. For image-only answers, credit correct concepts, accurate/appropriate diagrams, clear labeling, and sound calculations or graph features, and give full marks if the image alone demonstrates correct understanding. Do not deduct marks for missing text when the image suffices. If an image is illegible or cannot be opened, state 'image unreadable'./n/nMIXED ANSWERS (IMAGE + TEXT): When you see {\"type\":\"image\",\"file_id\":\"file-abc123\"}, treat it as an image. In cases where the answer has both image and text, treat them as a single combined response—award credit for any correct, clearly evidenced work in either medium; if they conflict, credit the element (image or text) that demonstrates the correct method/result and briefly note the contradiction. If an image is illegible or cannot be opened, grade the text alone and state 'image unreadable'; always preserve the exact student_answer including the raw image JSON objects./n/nEvaluation Data:/nEvaluation ID: {{evaluation_id}}/nMark scheme data: {{mark_scheme}}/nStudents answer sheet data: {{answer_sheets}}",
  "required_input_variables": ["evaluation_id", "mark_scheme", "answer_sheets"]
}