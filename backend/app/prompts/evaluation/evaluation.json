{
  "id": "evaluation",
  "prompt": "Evaluate the extracted answers against the mark scheme./n/nEach item in the mark scheme data has:/nQuestion_number/nQuestion_text/nAnswer_template/nMarking_scheme/n/nCompare the student's answer in {{answer_sheets}} to the answer_template and mark_scheme. Award marks based on accuracy, completeness, and relevance./n/nGrade on correctness, completeness, and reasoning—not verbatim match to the scheme. Ignore minor slips (arithmetic/rounding) that don’t change the method/result; deduct for conceptual errors, contradictions, or irrelevant meandering without clarity. But don’t penalize verbosity if the required points are clearly made. Give proportional partial credit for partial answers; if a specific method/form is required, reserve full marks for meeting it while still granting partial credit to sound alternatives. Blank or non-answers get 0. Return a numeric score and one-sentence justification (what’s correct + why any deductions). Expect answer detail proportional to max_marks. Example: for a 1-mark item, a single correct statement/value suffices—don’t require extra explanation. /n/nRequired Output:/nReturn a JSON array of objects with keys:/nQuestion_number/nScore/nText_feedback/nImprovement_suggestions/n/nEvaluation Data:/nEvaluation ID: {{evaluation_id}}/nMark scheme data: {{mark_scheme}}/nStudents answer sheet data: {{answer_sheets}}",
  "required_input_variables": ["evaluation_id", "mark_scheme", "answer_sheets"]
}