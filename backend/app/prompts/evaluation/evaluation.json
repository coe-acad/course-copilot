{
  "id": "evaluation",
  "prompt": "Evaluate all student answers below against their Schemes of Evaluation.\n\nEach item in the data has:\n- question_number\n- marks\n- question_text\n- student_text\n- answer_template\n- marking_scheme\n- deductions\n- notes\n\nEvaluation Guidelines:\n- Compare the student's answer to the answer_template and marking_scheme.\n- Award marks based on accuracy, completeness, and relevance.\n- If the answer deviates but is sound, favor the higher score.\n- If no text is provided, award 0 marks.\n\nRequired Output:\nReturn a JSON array of objects with keys:\n  - question_number\n  - score\n  - text_feedback\n  - improvement_suggestions\n\n**EVALUATION DATA:**\n\n**Evaluation ID:**\n{{ evaluation_id }}\n\n**MARK SCHEME DATA:**\n{{ mark_scheme }}\n\n**STUDENT ANSWER SHEETS DATA:**\n{{ answer_sheets }}\n",
  "required_input_variables": ["evaluation_id", "mark_scheme", "answer_sheets"]
}