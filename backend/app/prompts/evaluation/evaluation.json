{
  "id": "evaluation",
  "prompt": "Evaluate all student answers below against their Schemes of Evaluation.\n\nEach item in the data has:\n- question_number\n- marks\n- question_text\n- student_text\n- answer_template\n- marking_scheme\n- deductions\n\nEvaluation Guidelines:\n- Compare the student's answer to the answer_template and marking_scheme.\n- Award marks based on accuracy, completeness, and relevance.\n- If the answer deviates but is sound, favor the higher score.\n\n any one comprehensive, context-relevant answer can earn full marks.\n\nMinor calculation errors should not cost any marks if reasoning is sound.\n\n Provide marks liberally if the student shows rich understanding even if not using the right terms.\n\n***\n- If no text is provided, award 0 marks.\n\nRequired Output:\nReturn a JSON array of objects with keys:\n  - question_number\n  - score\n  - text_feedback\n  - improvement_suggestions\n\n**EVALUATION DATA:**\n\n**Evaluation ID:**\n{{ evaluation_id }}\n\n**MARK SCHEME DATA:**\n{{ mark_scheme }}\n\n**STUDENT ANSWER SHEETS DATA:**\n{{ answer_sheets }}\n",
  "required_input_variables": ["evaluation_id", "mark_scheme", "answer_sheets"]
}