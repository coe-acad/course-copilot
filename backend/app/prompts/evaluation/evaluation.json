{
  "id": "evaluation",
  "prompt": "Evaluate all student answers below against their Schemes of Evaluation.\n\nEach item in the data has:\n- question_number\n- marks\n- question_text\n- student_text\n- answer_template\n- marking_scheme\n- deductions\n\nEvaluation Guidelines:\n- Compare the student's answer to the answer_template and marking_scheme.\n- Grade on correctness, completeness, and reasoning—not verbatim match to the scheme. Award full marks to any valid, well-justified alternative; credit clear understanding even with imperfect terms. Ignore minor slips (arithmetic/rounding) that don’t change the method/result; deduct for conceptual errors, contradictions, or irrelevant padding. Give proportional partial credit; if a specific method/form is required, reserve full marks for meeting it while still granting partial credit to sound alternatives. Blank or non-answers get 0. In close calls, prefer generosity when reasoning aligns with the question. Return a numeric score and one-sentence justification (what’s correct + why any deductions).\n\nRequired Output:\nReturn a JSON array of objects with keys:\n  - question_number\n  - score\n  - text_feedback\n  - improvement_suggestions\n\n**EVALUATION DATA:**\n\n**Evaluation ID:**\n{{ evaluation_id }}\n\n**MARK SCHEME DATA:**\n{{ mark_scheme }}\n\n**STUDENT ANSWER SHEETS DATA:**\n{{ answer_sheets }}\n",
  "required_input_variables": ["evaluation_id", "mark_scheme", "answer_sheets"]
}